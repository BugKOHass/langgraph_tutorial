{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello Hassaan, it's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Assistant: I don't know your name. I'm a large language model, I don't have the ability to know your personal information or recall previous conversations. Each time you interact with me, it's a new conversation. If you'd like to share your name, I'd be happy to chat with you and use it in our conversation!\n",
      "Assistant: Today's date is March 22, 2025.\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_groq import ChatGroq\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "groq_api_key = \"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\"\n",
    "\n",
    "# * The class \"State\" is a dict based on TypedDict; this means that we can specify\n",
    "# which keys and what \"type\" of values it can have. We have defined it to have only \n",
    "# one key i.e. \"messages\" and the type of its value to be a list.\n",
    "# * The `add_messages` function in the annotation defines how this state should be updated:\n",
    "# we can only append messages to the list, not overwrite them.\n",
    "# * However, in our current implementation, we are not saving the state anywhere; when we\n",
    "# execute graph.stream(...), it initiates the graph with a new state.\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.3,\n",
    "    groq_api_key = groq_api_key\n",
    ")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # print(state)\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# * The first argument is the unique node name\n",
    "# * The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass\n",
    "\n",
    "\n",
    "# * In this case, we have only one node - hence, we would get only one output / event from graph.stream;\n",
    "# but if we had multiple nodes, we would get multiple outputs / events i.e. one output for each node.\n",
    "# * \"event\" is a dictionary containing the output of a node; its key is the name of that node e.g. \"chatbot\" and\n",
    "# its values are the list of dictionaries returned by the chatbot function.\n",
    "# * Our \"chatbot\" function is returning only a single dictionary, hence the list \"event.values()\" contains a\n",
    "# single dictionary: {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "# * In the dictionary {\"messages\": [llm.invoke(state[\"messages\"])]}, the value stored against the \"messages\" key \n",
    "# is a list; in our case, we can see that this list contains a single item only i.e. the response of the llm.\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    try:\n",
    "        # user_input = input(\"User: \")\n",
    "        # if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        #     print(\"Goodbye!\")\n",
    "        #     break\n",
    "        if i==0: user_input = \"My name is Hassaan.\"\n",
    "        if i==1: user_input = \"What's my name?\"\n",
    "        if i==2: user_input = \"What's the date today?\" # llama3.3-70b hosted on groq fetches the actual date!\n",
    "        stream_graph_updates(user_input)\n",
    "        if i==2: break\n",
    "        i += 1\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x28c6f951c10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2(a) - using custom BasicToolNode and route_tools (instead of the prebuilt ToolNode and tools_condition, respectively) - for learning purposes\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import ToolMessage\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "groq_api_key = \"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\"\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.3,\n",
    "    groq_api_key = groq_api_key\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        \n",
    "        # This creates a dictionary containing each tool name as the key and the tool itself as the value against that key\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        # := is the walrus operator\n",
    "        # It assigns the value inputs.get(\"messages\", []) to \"messages\" variable inside the \"if\" condition.\n",
    "        # If \"messages\" variable does not come out to be an empty list, the latest message (i.e. AIMessage) is assigned to\n",
    "        # \"message\" variable. The \"chatbot\" we defined above outputs only a single msg i.e. the response of the LLM (llm_with_tools).\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        # The LLM's tool-calling support and binding the llm to the tools (llm.bind_tools - see above) results in tool_calls being added to the AIMEssage.\n",
    "        # Each tool call contains the name of the tool that the LLM wants to call, the input (\"args\") to be fed to that tool as well as an ID for that tool call.\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            # For each tool call, we get the output of the relevant tool and append it to the \"outputs\" list.\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "# We initialize the BasicToolNode class using the list of tools (see __init__ method of the class BasicToolNode)\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Recall that edges route the control flow from one node to the next. Conditional edges usually contain \"if\" statements to \n",
    "# route to different nodes depending on the current graph state. These functions receive the current graph state and return \n",
    "# a string or list of strings indicating which node(s) to call next. In our case, we are returning just a string i.e. either\n",
    "# \"tools\" or END.\n",
    "# Below, define a router function called route_tools, that checks for tool_calls in the chatbot's output. Provide this \n",
    "# function to the graph by calling add_conditional_edges, which tells the graph that whenever the chatbot node completes, \n",
    "# check this function to see where to go next.\n",
    "# The condition will route to tools if tool calls are present and END if not.\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge (add_conditional_edges) to route to the tool node(BasicToolNode) if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    # This function gets, as an input, the output of the \"chatbot\" node - which has the same type as the class \"State\".\n",
    "    # The first condition (if isinstance) would have held true if we had defined the chatbot node's output to be a list. \n",
    "    # However, we have defined its output to be a dictionary whose key is \"messages\" and whose value is a list \n",
    "    # containing a single message i.e. the LLM's response.\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "# The `route_tools` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node.\n",
    "    # It defaults to the identity function (i.e. value=key), but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
